{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clean_code_enhance_multitask.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We provided easy to use notebook to replicate the results. \n",
        "Please refer to the corresponding author if there is any issue.\n",
        "\n"
      ],
      "metadata": {
        "id": "znVTIOW8DsoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the data\n"
      ],
      "metadata": {
        "id": "JpsZVjOA6Hmq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9V8vMRB587q"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import statistics\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "INPUT_DIR = ##### add the directory to the data\n",
        "seed = 42\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "# Torch RNG\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "# Python RNG\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "WINDOW_SIZE = 160\n",
        "NUMBER_MODELS = 1\n",
        "# some utility functions \n",
        "\n",
        "def normalize_z(x, avg, std_dev, smooth = 0.0001):\n",
        "  return (x-avg) / (std_dev + smooth)\n",
        "\n",
        "def normalize_mm(x, minn, maxx, smooth = 0.0001):\n",
        "  return 2*(x-(minn+maxx)/2) / (maxx - minn + smooth)\n",
        "\n",
        "\n",
        "def make_autopct(values):\n",
        "    def my_autopct(pct):\n",
        "        total = sum(values)\n",
        "        val = int(round(pct*total/100.0))\n",
        "\n",
        "        val = int(val / 1000)\n",
        "        return '{p:.2f}%  ({v:d}K)'.format(p=pct,v=val)\n",
        "    return my_autopct\n",
        "\n",
        "# Defining a function for plotting training and validation learning curves\n",
        "def plot_history(history):\n",
        "\t  # plot loss\n",
        "    plt.title('Loss')\n",
        "    plt.plot(history.history['loss'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_loss'], color='red', label='test')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "    plt.show()\n",
        "    \n",
        "    # plot accuracy\n",
        "    plt.title('Accuracy')\n",
        "    plt.plot(history.history['categorical_accuracy'], color='blue', label='train')\n",
        "    plt.plot(history.history['val_categorical_accuracy'], color='red', label='test')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'])\n",
        "    plt.show()\n",
        "\n",
        "def prep_label(y_x):\n",
        "  label=[]\n",
        "  for i in range(len(y_x)):\n",
        "    # print(i)\n",
        "    try:\n",
        "      label.append(statistics.mode(y_x[i]))\n",
        "    except:\n",
        "      try:\n",
        "        if statistics.mode(y_x[i-1])==statistics.mode(y_x[i+1]):\n",
        "           label.append(statistics.mode(y_x[i+1]))                 \n",
        "        else:\n",
        "           label.append(statistics.mode(y_x[i-1])) \n",
        "      except:   \n",
        "        label.append(statistics.mode(y_x[i-2]))\n",
        "  return np.array(label)\n",
        "\n",
        "# load data\n",
        "from sklearn.model_selection import  train_test_split\n",
        "\n",
        "dataset = np.load(os.path.join(INPUT_DIR, \"dataset.npy\"))\n",
        "x, y =dataset[:,:, :7], dataset[:,:, 7]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
        "phase_train = x_train[:,:,6] #phase\n",
        "phase_test=x_test[:,:,6]  #phase\n",
        "\n",
        "mean = np.load(os.path.join(INPUT_DIR, \"x_mean.npy\"))\n",
        "std = np.load(os.path.join(INPUT_DIR, \"x_stddev.npy\"))\n",
        "\n",
        "\n",
        "x_train = normalize_z(x_train[:,:,:6], mean, std)\n",
        "x_test = normalize_z(x_test[:,:,:6], mean, std)\n",
        "\n",
        "\n",
        "x_train1 = x_train #raw input\n",
        "y_train1 = y_train #labels\n",
        "\n",
        "x_test1 = x_test #raw input\n",
        "y_test1 = y_test  #labels\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(y_train1)\n",
        "y_test = to_categorical(y_test1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding Functions:\n",
        "\n",
        "\n",
        "*   One-hot encoding \n",
        "*   Gaussian encoding\n",
        "\n"
      ],
      "metadata": {
        "id": "oO6_jG_R7F8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "#### Gaussian encoding\n",
        "\n",
        "def gauss_fit(phase):\n",
        "  from scipy.stats import norm\n",
        "  aa=np.nonzero(phase)\n",
        "  sigma =.1\n",
        "  x1 = 0\n",
        "  x2 = 8\n",
        "  out=0\n",
        "  for mu in aa[0]:\n",
        "\n",
        "   z1 = ( x1 - mu ) / sigma\n",
        "   z2 = ( x2 - mu ) / sigma\n",
        "   x = np.arange(z1, z2, 10) # range of x in spec\n",
        "   x_all = np.arange(-10, 10, 0.001) # entire range of x, both in and out of spec\n",
        "   y = norm.pdf(x,0,1)\n",
        "   out+=y\n",
        "  return out\n",
        "\n",
        "#### One-hot encoding\n",
        "\n",
        "def encode_angle(angle, N):\n",
        "\n",
        "        num_bins = N\n",
        "        idx = int(angle / np.pi * num_bins) % num_bins\n",
        "        one_hot = np.zeros(num_bins)\n",
        "        one_hot[idx] = 1\n",
        "        return one_hot"
      ],
      "metadata": {
        "id": "n6epu1KC7UmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### train data\n",
        "ag=phase_train\n",
        "num_bins=8 #number of bins\n",
        "bb1=np.zeros((ag.shape[0],ag.shape[1],num_bins))\n",
        "for j in range(ag.shape[0]):\n",
        " for i in range(ag.shape[1]):\n",
        "  bb1[j,i,:]=gauss_fit(encode_angle(ag[j,i],num_bins))   # one-hot encoding can simply be achieved by removing the gauss_fit\n",
        "\n",
        "#### validation data\n",
        "ag=phase_test\n",
        "num_bins=8\n",
        "bb2=np.zeros((ag.shape[0],ag.shape[1],num_bins))\n",
        "for j in range(ag.shape[0]):\n",
        " for i in range(ag.shape[1]):\n",
        "  bb2[j,i,:]=gauss_fit(encode_angle(ag[j,i],num_bins))\n",
        "\n",
        "x_train1=np.concatenate((x_train,bb1),axis=2) # adding encoded phase information to the input\n",
        "x_test1=np.concatenate((x_test,bb2),axis=2)"
      ],
      "metadata": {
        "id": "8Z_uIhkH8Ksl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the data for training"
      ],
      "metadata": {
        "id": "-ZvHBW8290DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "bs = 128\n",
        "x_train1= np.transpose(x_train1,(0,2,1))\n",
        "x_test1= np.transpose(x_test1,(0,2,1))\n",
        "x_train2= torch.from_numpy(x_train1)\n",
        "y_train2= torch.from_numpy(y_train)\n",
        "phase_train=torch.from_numpy(phase_train)\n",
        "x_test2= torch.from_numpy(x_test1)\n",
        "y_test2= torch.from_numpy(y_test)\n",
        "phase_test=torch.from_numpy(phase_test)\n",
        "train_ds = TensorDataset(x_train2, y_train2,phase_train)\n",
        "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "\n",
        "test_ds = TensorDataset(x_test2, y_test2,phase_test)\n",
        "test_dl = DataLoader(test_ds, batch_size=bs)"
      ],
      "metadata": {
        "id": "N5B2lM1K9zb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function"
      ],
      "metadata": {
        "id": "0pSDIUqY-Dmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "def weighted_crossentropy(y_true, y_pred):    #### weighted crossentropy\n",
        "  EPSILON = 0.0001\n",
        "  # pdb.set_trace()\n",
        "  uu=torch.broadcast_to(torch.Tensor([2, 1, 1, 2]), y_true.shape).to(device)\n",
        "  vv=torch.broadcast_to(torch.Tensor([1, 1, 1, 1]), y_pred.shape).to(device)\n",
        "  # pdb.set_trace()\n",
        "  y_t = torch.multiply(uu , y_true).to(device)\n",
        "  h_t = torch.multiply(vv , y_pred).to(device)\n",
        "  # pdb.set_trace()\n",
        "  return torch.mean(- torch.multiply(y_t  , torch.log(torch.add(h_t, EPSILON))))\n",
        "\n",
        "def loss_batch_class(model, xb, yb,phb, opt=None):\n",
        "    xb1=xb[:,:,:80] # the first half of the input\n",
        "    x_recon, phase, y= model(xb1.float())\n",
        "    mse= nn.MSELoss()\n",
        "    l1_loss = nn.L1Loss()\n",
        "    class_w=0\n",
        "    lam=0.001\n",
        "    lam1=0.1 \n",
        "    recon_loss =mse(xb.float(),x_recon )  # reconstruction loss\n",
        "\n",
        "    x_cos =phase[:,0] # cosine axis of the phase\n",
        "    x_sin =phase[:,1] # sine axis of the phase\n",
        "\n",
        "\n",
        "    ## middle phase\n",
        "    eee1= (torch.cos(phb[:,80].float())).view(-1,1) # true cosine phase\n",
        "    eee2= (torch.sin(phb[:,80].float())).view(-1,1) # true sine phase\n",
        "    \n",
        "    rr= torch.cat([eee1,eee2],axis=1) #128*2\n",
        "    rr_cos= rr[:,0]/torch.norm(rr,dim=1) \n",
        "    rr_sin= rr[:,1]/torch.norm(rr,dim=1) \n",
        "    \n",
        "    g_phase=torch.cat([rr_cos.view(-1,1), rr_sin.view(-1,1)],dim=1)\n",
        "    es_phase=torch.cat([x_cos.view(-1,1), x_sin.view(-1,1)],dim=1)\n",
        "\n",
        "    loss2=mse(g_phase,es_phase) # phase reconstruction\n",
        "\n",
        "    circ=torch.norm(es_phase,dim=1) \n",
        "    loss3=mse(circ,torch.ones_like(circ)) #||X-1|| make sure that the point are close to the unit circle\n",
        "\n",
        "    loss= recon_loss+lam*loss2+lam1*loss3 +class_w*weighted_crossentropy(yb[:,:,:],y) # full loss\n",
        "\n",
        "    if opt is not None:\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "    return loss.item(), len(xb)"
      ],
      "metadata": {
        "id": "RayGnLw--C2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model "
      ],
      "metadata": {
        "id": "uglqqEIf_Wq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdiffeq"
      ],
      "metadata": {
        "id": "paF6ym7w_FyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "dim=64\n",
        "# num_bins=16\n",
        "adjoint= False\n",
        "if adjoint:\n",
        "    from torchdiffeq import odeint_adjoint as odeint\n",
        "else:\n",
        "    from torchdiffeq import odeint\n",
        "def norm(dim):\n",
        "    \"\"\"\n",
        "    Group normalization to improve model accuracy and training speed.\n",
        "    \"\"\"\n",
        "    return nn.GroupNorm(min(32, dim), dim)\n",
        "class ConcatConv1d(nn.Module):\n",
        "    \"\"\"\n",
        "    1d convolution concatenated with time for usage in ODENet.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in, dim_out, kernel_size=3, stride=1, padding=0, bias=True, transpose=False):\n",
        "        super(ConcatConv1d, self).__init__()\n",
        "        module = nn.ConvTranspose1d if transpose else nn.Conv1d\n",
        "        self._layer = module(\n",
        "            dim_in + 1, dim_out, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "            bias=bias\n",
        "        )\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        tt = torch.ones_like(x[:, :1, :]) * t\n",
        "        ttx = torch.cat([tt, x], 1)\n",
        "        return self._layer(ttx)\n",
        "\n",
        "\n",
        "class ODEfunc(nn.Module):\n",
        "    \"\"\"\n",
        "    Network architecture for ODENet.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super(ODEfunc, self).__init__()\n",
        "        self.norm1 = norm(dim)\n",
        "        self.relu = nn.SELU(inplace=True)\n",
        "        self.conv1 = ConcatConv1d(dim, dim, 3, 1, 1)\n",
        "        self.norm2 = norm(dim)\n",
        "        self.conv2 = ConcatConv1d(dim, dim, 3, 1, 1)\n",
        "        self.norm3 = norm(dim)\n",
        "        # self.drop1 = nn.Dropout(.25)\n",
        "        # self.drop2 = nn.Dropout(.25)\n",
        "        self.nfe = 0    # Number of function evaluations\n",
        "\n",
        "    def forward(self, t, x):\n",
        "        self.nfe += 1\n",
        "        out = self.norm1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(t, x)\n",
        "        # out = self.drop1(out)\n",
        "        out = self.norm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(t, out)\n",
        "        out = self.norm3(out)\n",
        "        return out\n",
        "\n",
        "tol=1e-3\n",
        "class ODENet(nn.Module):\n",
        "    # def __init__(self, odefunc, rtol=1e-3, atol=1e-3):\n",
        "    #     super(ODENet, self).__init__()\n",
        "    #     self.odefunc = odefunc\n",
        "    #     self.integration_time = torch.tensor([0, 1]).float()\n",
        "    #     # self.rtol = rtol\n",
        "    #     # self.atol = atol\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     self.integration_time = self.integration_time.type_as(x)\n",
        "    #     out = odeint_adjoint(self.odefunc, x, self.integration_time, rtol1, rtol1)\n",
        "        # return out[1]\n",
        "    def __init__(self, odefunc):\n",
        "        super(ODENet, self).__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.integration_time = torch.tensor([0, 1]).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.integration_time = self.integration_time.type_as(x)\n",
        "        # pdb.set_trace()\n",
        "        out = odeint(self.odefunc, x, self.integration_time, rtol=tol, atol=tol)\n",
        "        # pdb.set_trace()\n",
        "        return out[1]\n",
        "\n",
        "    # Update number of function evaluations (nfe)\n",
        "    @property\n",
        "    def nfe(self):\n",
        "        return self.odefunc.nfe\n",
        "\n",
        "    @nfe.setter\n",
        "    def nfe(self, value):\n",
        "        self.odefunc.nfe = value\n",
        "\n",
        "class Reshape(nn.Module):\n",
        "    def __init__(self, *args):\n",
        "        super(Reshape, self).__init__()\n",
        "        self.shape = args\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(-1,160,4)\n",
        "class Flatten(nn.Module):\n",
        "    \"\"\"\n",
        "    Flatten feature maps for input to linear layer.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Flatten, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
        "        return x.view(-1, shape)\n",
        "\n",
        "class classifier(nn.Module):\n",
        "        def __init__(self):\n",
        "          super(classifier, self).__init__()\n",
        "          self.norm1=norm(dim)\n",
        "          self.relu=nn.ReLU(inplace=True)\n",
        "          self.avg=nn.AdaptiveAvgPool1d(1)\n",
        "          self.fc_layer1=nn.Linear(dim, 160*4)\n",
        "          self.softmax=nn.Softmax()\n",
        "          self.flatten=Flatten()\n",
        "          self.reshape=Reshape()\n",
        "\n",
        "        def forward(self, x):\n",
        "          x=self.norm1(x)\n",
        "          x=self.relu(x)\n",
        "          x=self.avg(x)\n",
        "          x=self.flatten(x)\n",
        "          x= self.fc_layer1(x)\n",
        "          x=self.reshape(x)\n",
        "          return self.softmax(x)\n",
        "\n",
        "class Latent(nn.Module):\n",
        "        def __init__(self):\n",
        "          super(Latent, self).__init__()\n",
        "          self.norm1=norm(dim)\n",
        "          self.relu=nn.ReLU(inplace=True)\n",
        "          self.avg= nn.AdaptiveAvgPool1d(1)\n",
        "          self.flatten=Flatten()\n",
        "          self.linear=nn.Linear(dim, 2)\n",
        "        def forward(self, x):\n",
        "          x=self.norm1(x)\n",
        "          x=self.relu(x)\n",
        "          x=self.avg(x)\n",
        "          x=self.flatten(x)\n",
        "          x=self.linear(x)\n",
        "          return x\n",
        "\n",
        "\n",
        "class downsampling_layers(nn.Module):\n",
        "        def __init__(self):\n",
        "          super(downsampling_layers, self).__init__()\n",
        "          self.conv1=nn.Conv1d(6+8, dim, 3, 1)\n",
        "          self.norm1=norm(dim)\n",
        "          self.selu=nn.SELU(inplace=False)\n",
        "          self.conv2=nn.Conv1d(dim, dim, 4, 2, 1)\n",
        "          self.norm2=norm(dim)\n",
        "          self.conv3=nn.Conv1d(dim, dim, 3, 2, 1) # 4 to 2 for forcasting 80 window\n",
        "\n",
        "        def forward(self, x):\n",
        "\n",
        "          x=self.conv1(x)\n",
        "          x=self.norm1(x)\n",
        "          x=self.selu(x)\n",
        "\n",
        "          x=self.conv2(x)\n",
        "          x=self.norm2(x)\n",
        "          x=self.selu(x)\n",
        "\n",
        "          x=self.conv3(x)\n",
        "          return x\n",
        " \n",
        "class Decoder(nn.Module):\n",
        "        def __init__(self):\n",
        "          super(Decoder, self).__init__()\n",
        "          self.conv1=nn.ConvTranspose1d(64,32,11,1)\n",
        "          # self.upsample1=nn.Upsample(scale_factor=4, mode='nearest')\n",
        "          self.norm1= norm(32)\n",
        "          self.selu=nn.SELU(inplace=False)\n",
        "          self.conv2= nn.ConvTranspose1d(32,64,10,1)\n",
        "          self.norm2= norm(64)\n",
        "          self.conv3= nn.ConvTranspose1d(64,64,10,1)\n",
        "          self.norm3= norm(64)\n",
        "          self.conv4= nn.ConvTranspose1d(64,64,4,1) #(4 to 10) for whole window\n",
        "          self.norm4= norm(64)\n",
        "          self.conv5= nn.ConvTranspose1d(64,6+8,10,3) # (10 to 8)(3 to 2) for whole window\n",
        "          # self.upsample2=nn.Upsample(scale_factor=10, mode='nearest')\n",
        "          self.norm5= norm(6+8)    \n",
        "\n",
        "        def forward(self, x):\n",
        "          x=self.conv1(x)\n",
        "          x=self.norm1(x)\n",
        "          x=self.selu(x)\n",
        "\n",
        "          x=self.conv2(x)\n",
        "          x=self.norm2(x)\n",
        "          x=self.selu(x)\n",
        "\n",
        "          x=self.conv3(x)\n",
        "          x=self.norm3(x)\n",
        "          x=self.selu(x)\n",
        "\n",
        "          x=self.conv4(x)\n",
        "          x=self.norm4(x)\n",
        "          x=self.selu(x)\n",
        "\n",
        "          x=self.conv5(x)\n",
        "          x=self.norm5(x)\n",
        "          x=self.selu(x)\n",
        "          return x\n",
        "    \n",
        "class Classification(nn.Module):\n",
        "      def __init__(self, down_samp,decoder,feature_layers, classifier=None,latent=None ):\n",
        "        super(Classification, self).__init__()\n",
        "        self.down_samp = down_samp\n",
        "        self.decoder=decoder\n",
        "        self.feature_layers=feature_layers\n",
        "        self.classifier = classifier\n",
        "        self.latent=latent\n",
        "                \n",
        "      def forward(self, x):\n",
        "        x_shape = self.down_samp(x) # encoder\n",
        "        x_phase=self.latent(x_shape)# mapping to 2d\n",
        "        y=self.classifier(x_shape) #classifier\n",
        "        x = self.feature_layers(x_shape)\n",
        "        y_recon =self.decoder(x)\n",
        "        \n",
        "        return  y_recon,x_phase, y it should be in the first place\n",
        "\n",
        "down_samp= downsampling_layers()\n",
        "feature_layers =ODENet(ODEfunc(dim))\n",
        "decoder=Decoder()\n",
        "decoder_class= classifier() \n",
        "phase_model=Latent()\n",
        "class_model=Classification(down_samp=down_samp,decoder=decoder, feature_layers=feature_layers,classifier=decoder_class,latent=phase_model)\n",
        "model_parameters11 = filter(lambda p: p.requires_grad, class_model.parameters())\n",
        "params_len = sum([np.prod(p.size()) for p in model_parameters11])\n",
        "print(params_len)"
      ],
      "metadata": {
        "id": "yKPs-YbU_all"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "VsWpiNNvAhjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
        "device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')\n",
        "num_batches = len(train_dl)\n",
        "import time\n",
        "import math\n",
        "epochs=150\n",
        "model1=class_model.to(device)\n",
        "val_tmp=math.inf\n",
        "loss_func=F.cross_entropy\n",
        "opt=optim.RMSprop(model1.parameters(), lr=.001)\n",
        "valid_dl=test_dl\n",
        "scheduler=ReduceLROnPlateau(opt, 'min',patience=5,verbose=True)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "        print(f\"Training... epoch {epoch + 1}\")\n",
        "        device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        model1.train()   # Set model to training mode\n",
        "        batch_count = 0\n",
        "        start = time.time()\n",
        "        for xb, yb,phb in train_dl:\n",
        "            batch_count += 1\n",
        "            curr_time = time.time()\n",
        "            percent = round(batch_count/len(train_dl) * 100, 1)\n",
        "            elapsed = round((curr_time - start)/60, 1)\n",
        "            tr_loss,_=loss_batch_class(model1.to(device), xb.to(device), yb.to(device),phb.to(device),opt)\n",
        "            print(\"Percent trained: {:.4f}| Time elapsed: {:.3f}| Loss: {:.4f} \".format(percent,elapsed,tr_loss))\n",
        "        model1.eval()    # Set model to validation mode\n",
        "        with torch.no_grad():\n",
        "            losses, nums = zip(\n",
        "                *[loss_batch_class(model1, xb.to(device), yb.to(device),phb.to(device)) for xb, yb,phb in valid_dl]\n",
        "            )\n",
        "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "        scheduler.step(val_loss)\n",
        "        if val_loss< val_tmp:\n",
        "  \n",
        "          PATH='/loss_0001_01_gauss_m2.pt'\n",
        "          torch.save(model1.state_dict(), PATH)\n",
        "          val_tmp=val_loss\n",
        "          print(val_loss)\n",
        "        if epoch%5==0:\n",
        "             eval_test(model1)"
      ],
      "metadata": {
        "id": "9PKZHUhXAhOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction and Inference"
      ],
      "metadata": {
        "id": "Yt7JOz8KA3z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from epftoolbox.evaluation import MASE, sMAPE, MAPE,MAE\n",
        "def rmse(predictions, targets):\n",
        "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
        "\n",
        "def smape(a, f):\n",
        "    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)\n",
        "def my_mode(np_array, return_counts=False):\n",
        "  uniques, counts = np.unique(np_array, return_counts=True)\n",
        "  # print(uniques, counts)\n",
        "  \n",
        "  cnts = [0,0,0,0]\n",
        "  for i in range(uniques.shape[0]):\n",
        "    cnts[i] = counts[i]\n",
        "  \n",
        "  counts = np.array(cnts)\n",
        "  mode_indx = np.argmax(counts)\n",
        "\n",
        "\n",
        "  if not return_counts:\n",
        "    return uniques[mode_indx]\n",
        "  return uniques[mode_indx], cnts\n",
        "\n",
        "\n",
        "def normalize_z(x, avg, std_dev, smooth = 0.0001):\n",
        "  return (x-avg) / (std_dev + smooth)\n",
        "\n",
        "def eval_test(model1):\n",
        " from sklearn.metrics import classification_report\n",
        " INPUT_DIR = \"/test\"\n",
        " NORMALIZE_DIR = \"/content/drive/MyDrive/DLNN-ProjC1-main/output/new_data_sampling\"\n",
        "\n",
        "\n",
        "\n",
        "# some constants\n",
        "\n",
        " X_NAMES = [\"accel_x\", \"accel_y\", \"accel_z\", \"gyro_x\", \"gyro_y\", \"gyro_z\"]\n",
        " TIME = [\"TIME\"]\n",
        " CLASS = [\"CLASS\"]\n",
        " SEQUENCE_SIZE = 160\n",
        " POST_PROCESS_WINDOW = 10\n",
        "#  model,_=get_model(adam=True)\n",
        " mean = np.load(os.path.join(NORMALIZE_DIR, \"x_mean.npy\"))\n",
        " std = np.load(os.path.join(NORMALIZE_DIR, \"x_stddev.npy\"))\n",
        " # loading all models\n",
        " MODEL_DIR = \"/content/drive/MyDrive/DLNN-ProjC1-main\"\n",
        " models = (model1.to('cpu')).eval()\n",
        " outputs = {}\n",
        "\n",
        "#  print(\"number of models: \", len(models))\n",
        " NUMBER_MODELS = 1 #len(models)\n",
        "\n",
        " for i in range(-1, NUMBER_MODELS):\n",
        "  outputs[i] = {}  \n",
        "\n",
        "#  print(models[0])\n",
        " # generating preds\n",
        "# import pdb \n",
        " try_in=torch.zeros(0)\n",
        " phase_out=torch.zeros(0)\n",
        " z_shape_out=torch.zeros(0)\n",
        " try_out=torch.zeros(0)\n",
        " for model_number in range(1):\n",
        "  for test_input_file in sorted(os.listdir(INPUT_DIR)):\n",
        "    prefix = test_input_file[:15]\n",
        "\n",
        "    axp=pd.read_csv(os.path.join(INPUT_DIR, test_input_file))\n",
        "    tt=axp[axp['0']==3]\n",
        "    print('actual shape:',axp.shape)\n",
        "    print('filtering the labels:',tt.shape)\n",
        "    test_input = np.array(tt)\n",
        "\n",
        "\n",
        "    test_rows = test_input.shape[0]\n",
        "\n",
        "    test_input_x = test_input[:, :6]\n",
        "    label= test_input[:, 6]\n",
        "    test_output_x = test_input[:, 7]\n",
        "\n",
        "    outputs[-1][test_input_file] = label[::4]\n",
        "\n",
        "    test_input_x = normalize_z(test_input_x, mean, std)\n",
        "    \n",
        "\n",
        "    done_index = 0\n",
        "    test_input_batch = []\n",
        "    out_batch=[]\n",
        "    while done_index + SEQUENCE_SIZE < test_rows:\n",
        "      test_input_batch.append(test_input_x[done_index: done_index + SEQUENCE_SIZE])\n",
        "      out_batch.append(test_output_x[done_index: done_index + SEQUENCE_SIZE])\n",
        "      done_index += SEQUENCE_SIZE\n",
        "    # pdb.set_trace()\n",
        "    test_input_batch.append(test_input_x[-SEQUENCE_SIZE:])\n",
        "    out_batch.append(test_output_x[-SEQUENCE_SIZE:])\n",
        "    test_input_batch = np.array(test_input_batch)  \n",
        "    out_batch = np.array(out_batch) \n",
        "\n",
        "    ag=out_batch\n",
        "    num_bins=8\n",
        "    bb3=np.zeros((ag.shape[0],ag.shape[1],num_bins))\n",
        "    for j in range(ag.shape[0]):\n",
        "     for i in range(ag.shape[1]):\n",
        "       bb3[j,i,:]=guass_fit(encode_angle(ag[j,i],num_bins))\n",
        "\n",
        "    test_input_batch= np.concatenate((test_input_batch,bb3),axis=2)\n",
        "    test_input_batch = np.transpose(test_input_batch,(0,2,1))\n",
        "    test_input_batch1=test_input_batch[:,:,:80]\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # pdb.set_trace()\n",
        "        recon1,phase1,_ = models((torch.Tensor(test_input_batch1).float())) \n",
        "        shape1=models.down_samp((torch.Tensor(test_input_batch1).float()))\n",
        "        try_out=torch.cat([try_out,recon1])\n",
        "        try_in=torch.cat([try_in,torch.from_numpy(test_input_batch).float()])\n",
        "        phase_out=torch.cat([phase_out,phase1])\n",
        "        z_shape_out=torch.cat([z_shape_out,shape1])\n",
        "\n",
        " print('RMSE total:',rmse(try_in[:,:,:], try_out[:,:,:]))\n",
        " print('RMSE Reconstruction:',rmse(try_in[:,:,:80], try_out[:,:,:80]))\n",
        " print('RMSE Forecast:',rmse(try_in[:,:,80:], try_out[:,:,80:]))\n",
        " print('sMAPE:',sMAPE(try_in[:,:,80:].numpy(), try_out[:,:,80:].numpy()))\n",
        " print('MAE:',MAE(try_in[:,:,80:].numpy(), try_out[:,:,80:].numpy()))\n",
        " print('1s:',rmse(try_in[:,:,:120], try_out[:,:,:120]))\n",
        " print('2s:',rmse(try_in[:,:,:], try_out[:,:,:]))\n",
        " \n",
        " return try_in, try_out,phase_out,z_shape_out\n",
        "device = torch.device('cuda:' + str(0) if torch.cuda.is_available() else 'cpu')\n",
        "model1=class_model.to(device)\n",
        "try_in, try_out,phase_out,z_shape_out=eval_test(model1)"
      ],
      "metadata": {
        "id": "g9_irksUA3Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping the embedding space to 3D"
      ],
      "metadata": {
        "id": "uizYZfp3B-qj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aa=z_shape_out.reshape(-1,40*64) # (batch, 64,40) encoder\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "y=pca.fit_transform(aa)\n",
        "\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "ax = plt.axes(projection='3d')\n",
        "wee=np.zeros(len(y))\n",
        "ax.scatter3D(y[:,0],y[:,1], y[:,2])"
      ],
      "metadata": {
        "id": "Tt2GwtZSCJBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning some the tail in the plot"
      ],
      "metadata": {
        "id": "2UcWftlNCTkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "# from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "\n",
        "xxx=pd.DataFrame({'x':y[:,0],'y':y[:,1],'z':y[:,2]})\n",
        "\n",
        "\n",
        "''' Detection '''\n",
        "# IQR\n",
        "Q1 = np.percentile(xxx['x'], 15,\n",
        "\t\t\t\tinterpolation = 'midpoint')\n",
        "\n",
        "Q3 = np.percentile(xxx['x'], 85,\n",
        "\t\t\t\tinterpolation = 'midpoint')# 35\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print(\"Old Shape: \", xxx.shape)\n",
        "\n",
        "# Upper bound\n",
        "upper = np.where(xxx['x'] >= (Q3+1.5*IQR))\n",
        "# Lower bound\n",
        "lower = np.where(xxx['x'] <= (Q1-1.5*IQR))\n",
        "\n",
        "''' Removing the Outliers '''\n",
        "xxx.drop(upper[0], inplace = True)\n",
        "xxx.drop(lower[0], inplace = True)\n",
        "\n",
        "print(\"New Shape: \", xxx.shape)\n",
        "\n",
        "\n",
        "\n",
        "xn=xxx['x'].values\n",
        "yn=xxx['y'].values\n",
        "zn=xxx['z'].values\n",
        "wwr=np.array([xn,yn,zn]).T\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "ax = plt.axes(projection='3d')\n",
        "wee=np.zeros(len(y))\n",
        "ax.plot3D(wwr[:,0],wwr[:,1], wwr[:,2],'o')"
      ],
      "metadata": {
        "id": "1pKYzdAKCS27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting a circle to the circular shape area"
      ],
      "metadata": {
        "id": "SoRTtQMOCoBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import *\n",
        "from matplotlib.pyplot import *\n",
        "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
        "\n",
        "%matplotlib inline\n",
        "#-------------------------------------------------------------------------------\n",
        "# Generate points on circle\n",
        "# P(t) = r*cos(t)*u + r*sin(t)*(n x u) + C\n",
        "#-------------------------------------------------------------------------------\n",
        "def generate_circle_by_vectors(t, C, r, n, u):\n",
        "    n = n/linalg.norm(n)\n",
        "    u = u/linalg.norm(u)\n",
        "    P_circle = r*cos(t)[:,newaxis]*u + r*sin(t)[:,newaxis]*cross(n,u) + C\n",
        "    return P_circle\n",
        "\n",
        "def generate_circle_by_angles(t, C, r, theta, phi):\n",
        "    # Orthonormal vectors n, u, <n,u>=0\n",
        "    n = array([cos(phi)*sin(theta), sin(phi)*sin(theta), cos(theta)])\n",
        "    u = array([-sin(phi), cos(phi), 0])\n",
        "    \n",
        "    # P(t) = r*cos(t)*u + r*sin(t)*(n x u) + C\n",
        "    P_circle = r*cos(t)[:,newaxis]*u + r*sin(t)[:,newaxis]*cross(n,u) + C\n",
        "    return P_circle\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Generating circle\n",
        "#-------------------------------------------------------------------------------\n",
        "r = 2.5               # Radius\n",
        "C = np.mean(y,axis=0)#array([3,3,4])    # Center\n",
        "theta = 45/180*pi     # Azimuth\n",
        "phi   = -30/180*pi    # Zenith\n",
        "\n",
        "t = linspace(-2*pi, 2*pi, 100)\n",
        "P_gen = generate_circle_by_angles(t, C, r, theta, phi)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Cluster of points\n",
        "#-------------------------------------------------------------------------------\n",
        "t = linspace(-pi, -0.25*pi, 100)\n",
        "n = len(t)\n",
        "P = generate_circle_by_angles(t, C, r, theta, phi)\n",
        "\n",
        "# Add some random noise to the points\n",
        "P += random.normal(size=P.shape) * 0.1\n",
        "P=wwr# np.array([wee, y[:,1],y[:,2]]).T\n",
        "#-------------------------------------------------------------------------------\n",
        "# Plot\n",
        "#-------------------------------------------------------------------------------\n",
        "f, ax = subplots(1, 3, figsize=(15,5))\n",
        "alpha_pts = 0.5\n",
        "i = 0\n",
        "ax[i].plot(P_gen[:,0], P_gen[:,1], 'y-', lw=3, label='Generating circle')\n",
        "ax[i].scatter(P[:,0], P[:,1], alpha=alpha_pts, label='Cluster points P')\n",
        "ax[i].set_title('View X-Y')\n",
        "ax[i].set_xlabel('x'); ax[i].set_ylabel('y');\n",
        "ax[i].set_aspect('equal', 'datalim'); ax[i].margins(.1, .1)\n",
        "ax[i].grid()\n",
        "i = 1\n",
        "ax[i].plot(P_gen[:,0], P_gen[:,2], 'y-', lw=3, label='Generating circle')\n",
        "ax[i].scatter(P[:,0], P[:,2], alpha=alpha_pts, label='Cluster points P')\n",
        "ax[i].set_title('View X-Z')\n",
        "ax[i].set_xlabel('x'); ax[i].set_ylabel('z'); \n",
        "ax[i].set_aspect('equal', 'datalim'); ax[i].margins(.1, .1)\n",
        "ax[i].grid()\n",
        "i = 2\n",
        "ax[i].plot(P_gen[:,1], P_gen[:,2], 'y-', lw=3, label='Generating circle')\n",
        "ax[i].scatter(P[:,1], P[:,2], alpha=alpha_pts, label='Cluster points P')\n",
        "ax[i].set_title('View Y-Z')\n",
        "ax[i].set_xlabel('y'); ax[i].set_ylabel('z'); \n",
        "ax[i].set_aspect('equal', 'datalim'); ax[i].margins(.1, .1)\n",
        "ax[i].legend()\n",
        "ax[i].grid()\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# FIT CIRCLE 2D\n",
        "# - Find center [xc, yc] and radius r of circle fitting to set of 2D points\n",
        "# - Optionally specify weights for points\n",
        "#\n",
        "# - Implicit circle function:\n",
        "#   (x-xc)^2 + (y-yc)^2 = r^2\n",
        "#   (2*xc)*x + (2*yc)*y + (r^2-xc^2-yc^2) = x^2+y^2\n",
        "#   c[0]*x + c[1]*y + c[2] = x^2+y^2\n",
        "#\n",
        "# - Solution by method of least squares:\n",
        "#   A*c = b, c' = argmin(||A*c - b||^2)\n",
        "#   A = [x y 1], b = [x^2+y^2]\n",
        "#-------------------------------------------------------------------------------\n",
        "def fit_circle_2d(x, y, w=[]):\n",
        "    \n",
        "    A = array([x, y, ones(len(x))]).T\n",
        "    b = x**2 + y**2\n",
        "    \n",
        "    # Modify A,b for weighted least squares\n",
        "    if len(w) == len(x):\n",
        "        W = diag(w)\n",
        "        A = dot(W,A)\n",
        "        b = dot(W,b)\n",
        "    \n",
        "    # Solve by method of least squares\n",
        "    c = linalg.lstsq(A,b,rcond=None)[0]\n",
        "    \n",
        "    # Get circle parameters from solution c\n",
        "    xc = c[0]/2\n",
        "    yc = c[1]/2\n",
        "    r = sqrt(c[2] + xc**2 + yc**2)\n",
        "    return xc, yc, r\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# RODRIGUES ROTATION\n",
        "# - Rotate given points based on a starting and ending vector\n",
        "# - Axis k and angle of rotation theta given by vectors n0,n1\n",
        "#   P_rot = P*cos(theta) + (k x P)*sin(theta) + k*<k,P>*(1-cos(theta))\n",
        "#-------------------------------------------------------------------------------\n",
        "def rodrigues_rot(P, n0, n1):\n",
        "    \n",
        "    # If P is only 1d array (coords of single point), fix it to be matrix\n",
        "    if P.ndim == 1:\n",
        "        P = P[newaxis,:]\n",
        "    \n",
        "    # Get vector of rotation k and angle theta\n",
        "    n0 = n0/linalg.norm(n0)\n",
        "    n1 = n1/linalg.norm(n1)\n",
        "    k = cross(n0,n1)\n",
        "    k = k/linalg.norm(k)\n",
        "    theta = arccos(dot(n0,n1))\n",
        "    \n",
        "    # Compute rotated points\n",
        "    P_rot = zeros((len(P),3))\n",
        "    for i in range(len(P)):\n",
        "        P_rot[i] = P[i]*cos(theta) + cross(k,P[i])*sin(theta) + k*dot(k,P[i])*(1-cos(theta))\n",
        "\n",
        "    return P_rot\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# ANGLE BETWEEN\n",
        "# - Get angle between vectors u,v with sign based on plane with unit normal n\n",
        "#-------------------------------------------------------------------------------\n",
        "def angle_between(u, v, n=None):\n",
        "    if n is None:\n",
        "        return arctan2(linalg.norm(cross(u,v)), dot(u,v))\n",
        "    else:\n",
        "        return arctan2(dot(n,cross(u,v)), dot(u,v))\n",
        "\n",
        "    \n",
        "#-------------------------------------------------------------------------------\n",
        "# - Make axes of 3D plot to have equal scales\n",
        "# - This is a workaround to Matplotlib's set_aspect('equal') and axis('equal')\n",
        "#   which were not working for 3D\n",
        "#-------------------------------------------------------------------------------\n",
        "def set_axes_equal_3d(ax):\n",
        "    limits = array([ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()])\n",
        "    spans = abs(limits[:,0] - limits[:,1])\n",
        "    centers = mean(limits, axis=1)\n",
        "    radius = 0.5 * max(spans)\n",
        "    ax.set_xlim3d([centers[0]-radius, centers[0]+radius])\n",
        "    ax.set_ylim3d([centers[1]-radius, centers[1]+radius])\n",
        "    ax.set_zlim3d([centers[2]-radius, centers[2]+radius])\n",
        "%matplotlib inline\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# Init figures\n",
        "#-------------------------------------------------------------------------------\n",
        "fig = figure(figsize=(15,11))\n",
        "alpha_pts = 0.5\n",
        "figshape = (2,3)\n",
        "ax = [None]*4\n",
        "ax[0] = subplot2grid(figshape, loc=(0,0), colspan=2)\n",
        "ax[1] = subplot2grid(figshape, loc=(1,0))\n",
        "ax[2] = subplot2grid(figshape, loc=(1,1))\n",
        "ax[3] = subplot2grid(figshape, loc=(1,2))\n",
        "i = 0\n",
        "ax[i].set_title('Fitting circle in 2D coords projected onto fitting plane')\n",
        "ax[i].set_xlabel('x'); ax[i].set_ylabel('y');\n",
        "ax[i].set_aspect('equal', 'datalim'); ax[i].margins(.1, .1); ax[i].grid()\n",
        "i = 1\n",
        "ax[i].plot(P_gen[:,0], P_gen[:,1], 'y-', lw=3, label='Generating circle')\n",
        "ax[i].scatter(P[:,0], P[:,1], alpha=alpha_pts, label='Cluster points P')\n",
        "ax[i].set_title('View X-Y')\n",
        "ax[i].set_xlabel('x'); ax[i].set_ylabel('y');\n",
        "ax[i].set_aspect('equal', 'datalim'); ax[i].margins(.1, .1); ax[i].grid()\n",
        "i = 2\n",
        "ax[i].plot(P_gen[:,0], P_gen[:,2], 'y-', lw=3, label='Generating circle')\n",
        "ax[i].scatter(P[:,0], P[:,2], alpha=alpha_pts, label='Cluster points P')\n",
        "ax[i].set_title('View X-Z')\n",
        "ax[i].set_xlabel('x'); ax[i].set_ylabel('z'); \n",
        "ax[i].set_aspect('equal', 'datalim'); ax[i].margins(.1, .1); ax[i].grid()\n",
        "i = 3\n",
        "ax[i].plot(P_gen[:,1], P_gen[:,2], 'y-', lw=3, label='Generating circle')\n",
        "ax[i].scatter(P[:,1], P[:,2], alpha=alpha_pts, label='Cluster points P')\n",
        "ax[i].set_title('View Y-Z')\n",
        "ax[i].set_xlabel('y'); ax[i].set_ylabel('z'); \n",
        "ax[i].set_aspect('equal', 'datalim'); ax[i].margins(.1, .1); ax[i].grid()\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# (1) Fitting plane by SVD for the mean-centered data\n",
        "# Eq. of plane is <p,n> + d = 0, where p is a point on plane and n is normal vector\n",
        "#-------------------------------------------------------------------------------\n",
        "P_mean = P.mean(axis=0)\n",
        "P_centered = P - P_mean\n",
        "U,s,V = linalg.svd(P_centered)\n",
        "\n",
        "# Normal vector of fitting plane is given by 3rd column in V\n",
        "# Note linalg.svd returns V^T, so we need to select 3rd row from V^T\n",
        "normal = V[2,:]\n",
        "d = -dot(P_mean, normal)  # d = -<p,n>\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# (2) Project points to coords X-Y in 2D plane\n",
        "#-------------------------------------------------------------------------------\n",
        "P_xy = rodrigues_rot(P_centered, normal, [0,0,1])\n",
        "\n",
        "ax[0].scatter(P_xy[:,0], P_xy[:,1], alpha=alpha_pts, label='Projected points')\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# (3) Fit circle in new 2D coords\n",
        "#-------------------------------------------------------------------------------\n",
        "xc, yc, r = fit_circle_2d(P_xy[:,0], P_xy[:,1])\n",
        "\n",
        "#--- Generate circle points in 2D\n",
        "t = linspace(0, 2*pi, 300)\n",
        "xx = xc + r*cos(t)\n",
        "yy = yc + r*sin(t)\n",
        "\n",
        "ax[0].plot(xx, yy, 'k--', lw=2, label='Fitting circle')\n",
        "ax[0].plot(xc, yc, 'k+', ms=10)\n",
        "ax[0].legend()\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "# (4) Transform circle center back to 3D coords\n",
        "#-------------------------------------------------------------------------------\n",
        "C = rodrigues_rot(array([xc,yc,0]), [0,0,1], normal) + P_mean\n",
        "C = C.flatten()\n",
        "\n",
        "#--- Generate points for fitting circle\n",
        "t = linspace(0, 2*pi, 100)\n",
        "u = P[0] - C\n",
        "P_fitcircle = generate_circle_by_vectors(t, C, r, normal, u)\n",
        "\n",
        "ax[1].plot(P_fitcircle[:,0], P_fitcircle[:,1], 'k--', lw=2, label='Fitting circle')\n",
        "ax[2].plot(P_fitcircle[:,0], P_fitcircle[:,2], 'k--', lw=2, label='Fitting circle')\n",
        "ax[3].plot(P_fitcircle[:,1], P_fitcircle[:,2], 'k--', lw=2, label='Fitting circle')\n",
        "ax[3].legend()\n",
        "\n",
        "#--- Generate points for fitting arc\n",
        "u = P[0] - C\n",
        "v = P[-1] - C\n",
        "theta = angle_between(u, v, normal)\n",
        "\n",
        "t = linspace(0, theta, 100)\n",
        "P_fitarc = generate_circle_by_vectors(t, C, r, normal, u)\n",
        "\n",
        "ax[1].plot(P_fitarc[:,0], P_fitarc[:,1], 'k-', lw=3, label='Fitting arc')\n",
        "ax[2].plot(P_fitarc[:,0], P_fitarc[:,2], 'k-', lw=3, label='Fitting arc')\n",
        "ax[3].plot(P_fitarc[:,1], P_fitarc[:,2], 'k-', lw=3, label='Fitting arc')\n",
        "ax[1].plot(C[0], C[1], 'k+', ms=10)\n",
        "ax[2].plot(C[0], C[2], 'k+', ms=10)\n",
        "ax[3].plot(C[1], C[2], 'k+', ms=10)\n",
        "ax[3].legend()\n",
        "\n",
        "print('Fitting plane: n = %s' % array_str(normal, precision=4))\n",
        "print('Fitting circle: center = %s, r = %.4g' % (array_str(C, precision=4), r))\n",
        "print('Fitting arc: u = %s, Î¸ = %.4g' % (array_str(u, precision=4), theta*180/pi))\n",
        "\n",
        "%matplotlib\n",
        "\n",
        "fig = figure(figsize=(15,15))\n",
        "ax = fig.add_subplot(1,1,1,projection='3d')\n",
        "ax.plot(*P_gen.T, color='y', lw=3, label='Generating circle')\n",
        "ax.plot(*P.T, ls='', marker='o', alpha=0.5, label='Cluster points P')\n",
        "\n",
        "#--- Plot fitting plane\n",
        "xx, yy = meshgrid(linspace(0,6,11), linspace(0,6,11))\n",
        "zz = (-normal[0]*xx - normal[1]*yy - d) / normal[2]\n",
        "# ax.plot_surface(xx, yy, zz, rstride=2, cstride=2, color='y' ,alpha=0.2, shade=False)\n",
        "\n",
        "#--- Plot fitting circle\n",
        "ax.plot(*P_fitcircle.T, color='k', ls='--', lw=2, label='Fitting circle')\n",
        "ax.plot(*P_fitarc.T, color='k', ls='-', lw=3, label='Fitting arc')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "ax.legend()\n",
        "\n",
        "# ax.set_aspect('equal', 'datalim')\n",
        "set_axes_equal_3d(ax)\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "ax = plt.axes(projection='3d')\n",
        "wee=np.zeros(len(y))\n",
        "ax.plot3D(y[:,0],y[:,1], y[:,2],'o')\n",
        "ax.plot3D(P_fitcircle[:,0],P_fitcircle[:,1], P_fitcircle[:,2],'o')\n",
        "plt.savefig('/content/drive/MyDrive/DLNN-ProjC1-main/image_phase_AE/comb_lam04',dpi=100)"
      ],
      "metadata": {
        "id": "ci4WKqorCuk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mapping back to embedding space and recontruct the signal"
      ],
      "metadata": {
        "id": "vVwRfhHHCzIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_samp=P_fitcircle[::4]\n",
        "sst=pca.inverse_transform(x_samp)\n",
        "sst=sst.reshape(-1,64,40)\n",
        "aaa=model1.decoder(torch.from_numpy(sst).type(torch.FloatTensor)).detach().numpy()\n",
        "plt.figure(figsize=(10,8))\n",
        "# plt.subplot(221)\n",
        "\n",
        "# equivalent but more general\n",
        "ax1 = plt.subplot(2, 2, 1,projection='3d')\n",
        "ax1.view_init(30,45)\n",
        "\n",
        "ax2 = plt.subplot(2,2,2)\n",
        "ax1.plot3D(x_samp[:,0],x_samp[:,1], x_samp[:,2],'o')\n",
        "# ax=fig.add_subplot(1, 2, 1, projection='3d')\n",
        "for i in range(len(x_samp)):\n",
        "#  ax1.scatter(x_samp[:,0],x_samp[:,1], x_samp[:,2],'o')\n",
        " ax1.scatter(x_samp[i,0],x_samp[i,1], x_samp[i,2],'o')\n",
        "#  ax1.view_init(10,90)\n",
        "# plt.figure()\n",
        " ax2 = plt.subplot(2,2,2)\n",
        " ax2.plot(aaa[i,3,:])\n",
        " \n",
        " plt.savefig('/content/drive/MyDrive/DLNN-ProjC1-main/Updates/images'+str(i),dpi=100)\n",
        " ax2.cla()"
      ],
      "metadata": {
        "id": "HWYviwfHCyqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Peak detection function"
      ],
      "metadata": {
        "id": "LbwegnLFDNY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import signal\n",
        "def cal_def(peaks):\n",
        "  v=[]\n",
        "  for i in range(len(peaks)-1):\n",
        "    v.append(peaks[i+1]-peaks[i])\n",
        "  return np.array(v)\n",
        "\n",
        "def pick_right_peaks(peaks):\n",
        "  a= cal_def(peaks)\n",
        "  mean=np.mean(a)\n",
        "  print(mean)\n",
        "  i=0\n",
        "  val_peaks=[]\n",
        "  while i<len(a):\n",
        "    if a[i]<=.5*mean:\n",
        "      print('rejection:',a[i])\n",
        "      i+=1\n",
        "    else:\n",
        "      print('accepted:',a[i])\n",
        "      val_peaks.append(peaks[i]) \n",
        "      print(a[i])\n",
        "      i+=1\n",
        "  if a[-1]>=.5*mean:\n",
        "      val_peaks.append(peaks[-1])\n",
        "  return val_peaks\n",
        "\n",
        "\n",
        "for kr in range(len(total_fs1)):\n",
        " aa=np.load(total_fs1[kr],allow_pickle=True)['X'] # loading raw signals \n",
        " dd1=aa.flatten()\n",
        " time=np.linspace(0,len(dd1),len(dd1))\n",
        " sos = signal.butter(1, 15, 'hp', fs=1000, output='sos')\n",
        " filtered = signal.sosfilt(sos, dd1)\n",
        "\n",
        "\n",
        " from scipy.signal import find_peaks,find_peaks_cwt\n",
        " import numpy as np\n",
        " pp=1000\n",
        " peak_time=[]\n",
        " for i in range(int(len(dd1)/1000)):\n",
        "\n",
        "  m=i*1000\n",
        "  dd=filtered[m:m+pp]\n",
        "  tt= time[m:m+pp]\n",
        "  peaks, _ = find_peaks(dd, height=.3 )# setting a threshold\n",
        "  aaa=cal_def(peaks)\n",
        "  bb=np.mean(cal_def(peaks))\n",
        "  result1=[]\n",
        "\n",
        "  for j in range(len(aaa)):\n",
        "   if aaa[j]< .5*bb:\n",
        "     if dd[peaks[j]]>dd[peaks[j+1]]:\n",
        "        result1.append(j+1)\n",
        "     else:\n",
        "      #  print(i)\n",
        "        result1.append(j)\n",
        "   else:\n",
        "    print('pass')\n",
        "\n",
        "  result=result1\n",
        "\n",
        "  new=np.delete(peaks, list(np.array(result)), None)\n",
        "  if len(new)>4:\n",
        "    peak_time.append(tt[new[0:4]])\n",
        "  else:\n",
        "    peak_time.append(tt[new])\n",
        "\n",
        "#  peak_time.append(tt[new])\n",
        "\n",
        " from scipy.io import savemat\n",
        "# mdic = {\"times\": peak_time}\n",
        "# savemat(\"/content/peak_times.mat\", mdic)\n",
        "\n",
        "# peak_time\n",
        " flat_list = [item for sublist in peak_time for item in sublist]\n",
        " flat_times=flat_list\n",
        " imu_time=time\n",
        "\n",
        " phase= np.zeros(len(imu_time))\n",
        " for i in range(len(flat_times)-1):\n",
        "  idx = np.where((imu_time>flat_times[i])&(imu_time<flat_times[i+1]))\n",
        "  tp = 2*np.pi*(imu_time[idx]-flat_times[i])/(flat_times[i+1]-flat_times[i]);\n",
        "  phase[idx]=tp\n",
        "\n",
        " np.savez_compressed('/content/drive/MyDrive/ECG/filtered_phase/'+total_fs1[kir][47:], X=filtered, phase=phase)"
      ],
      "metadata": {
        "id": "GSmTRv7zDQ8H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}